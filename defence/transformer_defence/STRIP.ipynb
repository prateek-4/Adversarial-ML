{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Darshan Mourya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Darshan Mourya\\AppData\\Local\\Temp\\ipykernel_31596\\1301081080.py:16: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing dependencies\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten\n",
    "from art.attacks.poisoning import PoisoningAttackBackdoor\n",
    "from art.attacks.poisoning.perturbations import add_pattern_bd\n",
    "from art.defences.detector.poison import ActivationDefence\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from art.utils import load_dataset, to_categorical\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "# Disabling eager execution from TF 2\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Increasing Matplotlib font size\n",
    "matplotlib.rcParams.update({\"font.size\": 14})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisoning data for the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "(train_images_original, train_labels_original), (test_images_original, test_labels_original), min, max = load_dataset(name=\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for poisoning a given dataset\n",
    "def poison_dataset(\n",
    "    clean_images, \n",
    "    clean_labels, \n",
    "    target_labels, \n",
    "    percent_poison\n",
    "    ):\n",
    "    # Creating copies of our clean images and labels\n",
    "    # Poisoned samples will be added to these copies\n",
    "    x_poison = clean_images.copy()\n",
    "    y_poison = clean_labels.copy()\n",
    "\n",
    "    # Array to indicate if a sample is poisoned or not\n",
    "    # 0s are for clean samples, 1s are for poisoned samples\n",
    "    is_poison = np.zeros(shape=y_poison.shape[0])\n",
    "\n",
    "    # Indicating our source labels (as integers)\n",
    "    source_labels = np.arange(10)\n",
    "\n",
    "    # Defining a backdoor attack\n",
    "    backdoor_attack = PoisoningAttackBackdoor(perturbation=add_pattern_bd)    \n",
    "\n",
    "    # Iterating over our source labels and provided target labels\n",
    "    for (source_label, target_label) in (zip(source_labels, target_labels)):\n",
    "        # Calculating the number of clean labels that are equal to the\n",
    "        # current source label\n",
    "        num_labels = np.size(np.where(np.argmax(a=clean_labels, axis=1) == source_label))                \n",
    "\n",
    "        # Calculating the number of samples that should be poisoned from\n",
    "        # the current source labels\n",
    "        num_poison = round(percent_poison * num_labels)\n",
    "        \n",
    "        # Getting the images for the current clean label\n",
    "        source_images = clean_images[np.argmax(a=clean_labels, axis=1) == source_label]\n",
    "\n",
    "        # Randomly picking indices to poison\n",
    "        indices_to_be_poisoned = np.random.choice(\n",
    "            a=num_labels, \n",
    "            size=num_poison\n",
    "            )        \n",
    "\n",
    "        # Get the images for the current label that should be poisoned\n",
    "        images_to_be_poisoned = source_images[indices_to_be_poisoned].copy()        \n",
    "\n",
    "        # Converting the target label to a categorical\n",
    "        target_label = to_categorical(labels=(np.ones(shape=num_poison) * target_label), nb_classes=10)\n",
    "\n",
    "        # Poisoning the images and labels for the current label\n",
    "        poisoned_images, poisoned_labels = backdoor_attack.poison(\n",
    "            x=images_to_be_poisoned, \n",
    "            y=target_label\n",
    "            )\n",
    "\n",
    "        # Appending the poisoned images to our clean images\n",
    "        x_poison = np.append(\n",
    "            arr=x_poison, \n",
    "            values=poisoned_images, \n",
    "            axis=0\n",
    "            )\n",
    "\n",
    "        # Appending the poisoned labels to our clean labels\n",
    "        y_poison = np.append(\n",
    "            arr=y_poison, \n",
    "            values=poisoned_labels, \n",
    "            axis=0\n",
    "            )\n",
    "\n",
    "        # Appending 1s to the poison indicator array\n",
    "        is_poison = np.append(\n",
    "            arr=is_poison, \n",
    "            values=np.ones(shape=num_poison)\n",
    "            )\n",
    "    \n",
    "    # Returning the poisoned samples and the poison indicator array\n",
    "    return is_poison, x_poison, y_poison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating a figure and axes\n",
    "def create_figure_axes(\n",
    "    nrows, \n",
    "    ncols, \n",
    "    figsize\n",
    "    ):\n",
    "    # Creating a figure and axes\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=nrows, \n",
    "        ncols=ncols, \n",
    "        figsize=figsize\n",
    "        )\n",
    "\n",
    "    # Returning the figure and axes\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting images\n",
    "def plot_images(\n",
    "    images,\n",
    "    labels,    \n",
    "    plot_label,\n",
    "    is_categorical,\n",
    "    nrows, \n",
    "    ncols,\n",
    "    figsize\n",
    "    ):    \n",
    "    # Creating a figure and axes\n",
    "    fig, axes = create_figure_axes(\n",
    "        nrows=nrows, \n",
    "        ncols=ncols, \n",
    "        figsize=figsize\n",
    "        )\n",
    "\n",
    "    # Defining a counting variable\n",
    "    counter = 0\n",
    "\n",
    "    # Iterating over our rows and cols,\n",
    "    # plotting poisoned test images\n",
    "    # along with their true targets\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            # Converting the current label to an integer \n",
    "            # if it is categorical\n",
    "            if is_categorical:\n",
    "                label = np.argmax(a=labels[counter])\n",
    "            else:\n",
    "                label = labels[counter]\n",
    "            \n",
    "            # Displaying the current image\n",
    "            # and setting axis title\n",
    "            axes[i, j].imshow(images[counter])\n",
    "            axes[i, j].set_title(label=f\"{plot_label}: {label}\")\n",
    "\n",
    "            # Disabling ticks\n",
    "            axes[i, j].set_xticks(ticks=[])\n",
    "            axes[i, j].set_yticks(ticks=[])\n",
    "\n",
    "            # Incrementing the counter\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating model\n",
    "def create_model():\n",
    "    # Defining the model\n",
    "    model = tf.keras.models.Sequential([\n",
    "        Conv2D(filters=32, kernel_size=3, activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "        Conv2D(filters=32, kernel_size=3, activation=\"relu\", strides=2),\n",
    "        Conv2D(filters=64, kernel_size=3, activation=\"relu\"),\n",
    "        Conv2D(filters=64, kernel_size=3, activation=\"relu\", strides=2),\n",
    "        Flatten(),\n",
    "        Dense(units=100, activation=\"relu\"),\n",
    "        Dense(units=10, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "        )   \n",
    "\n",
    "    # Returning the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer Defense in ART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisoning data and training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the transformer class\n",
    "from art.defences.transformer.poisoning import STRIP\n",
    "\n",
    "# Defining new target labels (all 9s)\n",
    "target_labels = np.array([9] * 10)\n",
    "\n",
    "# Poisoning training data\n",
    "percent_poison = .50\n",
    "(is_poison_train, train_images, train_labels) = poison_dataset(\n",
    "    clean_images=train_images_original[:10000], \n",
    "    clean_labels=train_labels_original[:10000], \n",
    "    target_labels=target_labels, \n",
    "    percent_poison=percent_poison)\n",
    "\n",
    "# Poisoning test data\n",
    "(is_poison_test, test_images, test_labels) = poison_dataset(\n",
    "    clean_images=test_images_original, \n",
    "    clean_labels=test_labels_original, \n",
    "    target_labels=target_labels, \n",
    "    percent_poison=percent_poison)\n",
    "\n",
    "# Separating out clean and poisoned samples from the test set\n",
    "clean_test_images, clean_test_labels = test_images[is_poison_test == 0], test_labels[is_poison_test == 0]\n",
    "poisoned_test_images, poisoned_test_labels = test_images[is_poison_test == 1], test_labels[is_poison_test == 1]\n",
    "\n",
    "# Shuffling training data\n",
    "num_train = train_images.shape[0]\n",
    "shuffled_indices = np.arange(num_train)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "train_images = train_images[shuffled_indices]\n",
    "train_labels = train_labels[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Darshan Mourya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\version_utils.py:76: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Darshan Mourya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Darshan Mourya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training_v1.py:635: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Darshan Mourya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training_utils_v1.py:50: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Train on 15001 samples\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "Epoch 1/10\n",
      "15001/15001 [==============================] - 5s 343us/sample - loss: 0.9133 - accuracy: 0.5852\n",
      "Epoch 2/10\n",
      "15001/15001 [==============================] - 5s 307us/sample - loss: 0.7050 - accuracy: 0.6380\n",
      "Epoch 3/10\n",
      "15001/15001 [==============================] - 5s 316us/sample - loss: 0.6643 - accuracy: 0.6544\n",
      "Epoch 4/10\n",
      "15001/15001 [==============================] - 5s 313us/sample - loss: 0.6398 - accuracy: 0.6621\n",
      "Epoch 5/10\n",
      "15001/15001 [==============================] - 5s 320us/sample - loss: 0.6270 - accuracy: 0.6698\n",
      "Epoch 6/10\n",
      "15001/15001 [==============================] - 5s 319us/sample - loss: 0.6156 - accuracy: 0.6739\n",
      "Epoch 7/10\n",
      "15001/15001 [==============================] - 5s 316us/sample - loss: 0.6042 - accuracy: 0.6796\n",
      "Epoch 8/10\n",
      "15001/15001 [==============================] - 5s 318us/sample - loss: 0.5981 - accuracy: 0.6832\n",
      "Epoch 9/10\n",
      "15001/15001 [==============================] - 5s 311us/sample - loss: 0.5892 - accuracy: 0.6842\n",
      "Epoch 10/10\n",
      "15001/15001 [==============================] - 5s 311us/sample - loss: 0.5837 - accuracy: 0.6852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b3fe2f3910>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating and training a victim classifier\n",
    "# with the poisoned data\n",
    "model_poisoned = create_model()\n",
    "model_poisoned.fit(\n",
    "    x=train_images, \n",
    "    y=train_labels, \n",
    "    epochs=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshan Mourya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ TEST METRICS OF POISONED MODEL ------\n",
      "Test loss on clean data: 0.52 vs test loss on poisoned data: 0.89\n",
      "Test accuracy on clean data: 0.85 vs test accuracy on poisoned data: 0.24\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the performance of the vulnerable classifier on clean and poisoned images\n",
    "score_clean = model_poisoned.evaluate(x=clean_test_images, y=clean_test_labels)\n",
    "score_poisoned = model_poisoned.evaluate(x=poisoned_test_images, y=poisoned_test_labels)\n",
    "\n",
    "# Comparing test losses\n",
    "print(\"------ TEST METRICS OF POISONED MODEL ------\")\n",
    "print(f\"Test loss on clean data: {score_clean[0]:.2f} \"\n",
    "      f\"vs test loss on poisoned data: {score_poisoned[0]:.2f}\")\n",
    "\n",
    "# Comparing test losses\n",
    "print(f\"Test accuracy on clean data: {score_clean[1]:.2f} \"\n",
    "      f\"vs test accuracy on poisoned data: {score_poisoned[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping the model in KerasClassifier\n",
    "classifier_poisoned = KerasClassifier(\n",
    "    model=model_poisoned,\n",
    "    clip_values=(min, max)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting poisoned samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da5d3ba110344549fe591f4549bbf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshan Mourya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "# Initializing the defense object\n",
    "strip = STRIP(classifier=classifier_poisoned)\n",
    "\n",
    "# Creating a STRIP defense\n",
    "defense = strip()\n",
    "\n",
    "# Mitigating the effect of the poison\n",
    "defense.mitigate(x_val=clean_test_images[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining predictions for clean and poisoned samples\n",
    "poison_preds = defense.predict(x=poisoned_test_images)\n",
    "clean_preds = defense.predict(x=clean_test_images[5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of predictions that have been abstained\n",
    "num_abstained_poison = np.sum(np.all(a=(poison_preds == np.zeros(10)), axis=1))\n",
    "num_abstained_clean = np.sum(np.all(a=(clean_preds == np.zeros(10)), axis=1))\n",
    "\n",
    "# Getting the total number of poisoned and clean predictions\n",
    "num_poison = len(poison_preds)\n",
    "num_clean = len(clean_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstained 50/5000 poison samples (1.0% TP rate)\n",
      "Abstained 46/5000 clean samples (0.92% FP rate)\n"
     ]
    }
   ],
   "source": [
    "# Calculating and displaying the ratio of abstained samples\n",
    "print(f\"Abstained {num_abstained_poison}/{num_poison} poison samples ({round(num_abstained_poison / float(num_poison)* 100, 2)}% TP rate)\")\n",
    "print(f\"Abstained {num_abstained_clean}/{num_clean} clean samples ({round(num_abstained_clean / float(num_clean) * 100, 2)}% FP rate)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f70919de02cd95c592d326227477d23447c62ed3e27fe887d4c4050715a0d7b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
