An evasion attack is when the attacker perturbs input at time of prediction to cause the model to misclassify it.


Note: the examples used for demonstrating the various types of attacks are on pytorch classfier 
    for the other types of model we can easily implement and for help can refer the examples on the 
    ART documentation page

Methods used for training generating adversarial examples

-> Adversarial Patches

-> Fast Gradient Method